{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c657b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edad2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED   \n",
      "gemma3:270m                 e7d36fb2c3b3    291 MB    5 days ago    \n",
      "mxbai-embed-large:latest    468836162de7    669 MB    5 days ago    \n",
      "tinyllama:latest            2644915ede35    637 MB    5 days ago    \n",
      "nomic-embed-text:latest     0a109f422b47    274 MB    5 days ago    \n",
      "gpt-oss:120b-cloud          569662207105    -         5 days ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6cb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model = \"gemma3:270m\",\n",
    "    temperature= 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4de735",
   "metadata": {},
   "source": [
    "LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3eac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path = \"book\",\n",
    "    glob = \"*.pdf\",\n",
    "    loader_cls= PyPDFLoader\n",
    ")\n",
    "documents = loader.lazy_load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103ee9a",
   "metadata": {},
   "source": [
    "CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "509cc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128,\n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n, \\n, \" \", \"\", \"]\n",
    ")\n",
    "chunk = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea497b",
   "metadata": {},
   "source": [
    "EMBEDDING AND VECTOR DBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5180fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    return (\n",
    "        text\n",
    "        .encode(\"utf-8\", errors=\"ignore\")\n",
    "        .decode(\"utf-8\")\n",
    "    )\n",
    "\n",
    "for doc in chunk:\n",
    "    doc.page_content = clean_text(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f6aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(\n",
    "  model = \"nomic-embed-text:latest\",\n",
    ")\n",
    "persist_directory = \"./chroma_db\"\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunk,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"rev.ipynb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "202b0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RETRIVER\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    kwargs = {\"k\":4}\n",
    ")\n",
    "\n",
    "#OLLAMA\n",
    "llm = ChatOllama(\n",
    "    model = \"gemma3:270m\",\n",
    "    temperature= 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a528be56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain created successfully using LCEL!\n",
      "\n",
      "RAG Pipeline Flow:\n",
      "  1. User provides a query\n",
      "  2. Retriever finds top 4 relevant chunks (local ChromaDB)\n",
      "  3. Chunks are formatted as context\n",
      "  4. Context + question formatted with prompt template\n",
      "  5. Local LLM (gemma3:1b) generates answer\n",
      "  6. Answer parsed and returned\n",
      "\n",
      "ðŸ”’ Everything runs locally on your machine!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are a helpful assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer based on the context, say that you don't know. \"\n",
    "    \"Keep the answer concise and accurate.\\n\\n\"\n",
    "    \"Context: {context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "      \"context\" : retriever | format_docs,\n",
    "      \"question\" : RunnablePassthrough()\n",
    "    }  \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ RAG chain created successfully using LCEL!\")\n",
    "print(\"\\nRAG Pipeline Flow:\")\n",
    "print(\"  1. User provides a query\")\n",
    "print(\"  2. Retriever finds top 4 relevant chunks (local ChromaDB)\")\n",
    "print(\"  3. Chunks are formatted as context\")\n",
    "print(\"  4. Context + question formatted with prompt template\")\n",
    "print(\"  5. Local LLM (gemma3:1b) generates answer\")\n",
    "print(\"  6. Answer parsed and returned\")\n",
    "print(\"\\nðŸ”’ Everything runs locally on your machine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a27bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCE DOCUMENTS USED:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "  Page: 1\n",
      "  Content: Machine Learning For Absolute\n",
      "Beginners\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oliver Theobald...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "  Page: 12\n",
      "  Content: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "  Page: 13\n",
      "  Content: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "  Page: 25\n",
      "  Content: know. C and C++ are the default programming languages for advanced\n",
      "machine learning because they can run directly on a GPU (Graphical\n",
      "Processing Unit). Python needs to be converted first before it can...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is the main topic or contribution of this document?\"\n",
    "answer =rag_chain.invoke(query1)\n",
    "print(\"=\" * 80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(answer)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Show source documents\n",
    "print(\"\\nSOURCE DOCUMENTS USED:\")\n",
    "print(\"=\" * 80)\n",
    "retrieved_docs = retriever.invoke(query1)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"  Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
